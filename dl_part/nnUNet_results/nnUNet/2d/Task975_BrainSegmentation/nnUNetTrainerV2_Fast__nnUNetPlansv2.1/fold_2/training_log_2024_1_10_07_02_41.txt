Starting... 
2024-01-10 07:02:41.090295: Using splits from existing split file: /home/edalita/Documents/MAIA/3-Semestre/MIRMISAProject/IBSR18-tissue-segmentation/dl_part/nnUNet_preprocessed/Task975_BrainSegmentation/splits_final.pkl 
2024-01-10 07:02:41.090843: The split file contains 5 splits. 
2024-01-10 07:02:41.090935: Desired fold for training: 2 
2024-01-10 07:02:41.091014: This split has 12 training and 3 validation cases. 
2024-01-10 07:02:41.202128: TRAINING KEYS:
 odict_keys(['IBSR_01', 'IBSR_02', 'IBSR_03', 'IBSR_04', 'IBSR_05', 'IBSR_06', 'IBSR_07', 'IBSR_08', 'IBSR_09', 'IBSR_10', 'IBSR_12', 'IBSR_14']) 
2024-01-10 07:02:41.202385: VALIDATION KEYS:
 odict_keys(['IBSR_11', 'IBSR_13', 'IBSR_15']) 
2024-01-10 07:02:42.051844: loading checkpoint /home/edalita/Documents/MAIA/3-Semestre/MIRMISAProject/IBSR18-tissue-segmentation/dl_part/nnUNet_results/nnUNet/2d/Task975_BrainSegmentation/nnUNetTrainerV2_Fast__nnUNetPlansv2.1/fold_2/model_final_checkpoint.model train= True 
2024-01-10 07:02:42.553978: lr: 0.004384 
2024-01-10 07:02:55.357198: Unable to plot network architecture: 
2024-01-10 07:02:55.357376: failed to execute ['dot', '-Tpdf', '-O', '/home/edalita/Documents/MAIA/3-Semestre/MIRMISAProject/IBSR18-tissue-segmentation/dl_part/nnUNet_results/nnUNet/2d/Task975_BrainSegmentation/nnUNetTrainerV2_Fast__nnUNetPlansv2.1/fold_2/network_architecture'], make sure the Graphviz executables are on your systems' PATH 
2024-01-10 07:02:55.357503: 
printing the network instead:
 
2024-01-10 07:02:55.357611: Generic_UNet(
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(960, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (2): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (3): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
    (4): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (2): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (3): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (4): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(256, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
          (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (5): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): InstanceNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList()
  (tu): ModuleList(
    (0): ConvTranspose2d(480, 480, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (1): ConvTranspose2d(480, 256, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (3): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2), bias=False)
    (4): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2), bias=False)
  )
  (seg_outputs): ModuleList(
    (0): Conv2d(480, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): Conv2d(256, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (2): Conv2d(128, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (3): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (4): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
) 
2024-01-10 07:02:55.361626: 
 
2024-01-10 07:02:55.364229: 
epoch:  30 
2024-01-10 07:12:01.146614: train loss : -0.8693 
2024-01-10 07:12:34.785784: validation loss: -0.8424 
2024-01-10 07:12:34.786428: Average global foreground Dice: [0.9065, 0.9464, 0.9329] 
2024-01-10 07:12:34.786683: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:12:35.481392: lr: 0.004186 
2024-01-10 07:12:35.537086: saving checkpoint... 
2024-01-10 07:12:36.505662: done, saving took 1.02 seconds 
2024-01-10 07:12:36.506524: This epoch took 581.141953 s
 
2024-01-10 07:12:36.506608: 
epoch:  31 
2024-01-10 07:20:34.981994: train loss : -0.8692 
2024-01-10 07:21:08.769763: validation loss: -0.8451 
2024-01-10 07:21:08.770541: Average global foreground Dice: [0.906, 0.9463, 0.9333] 
2024-01-10 07:21:08.770676: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:21:09.374591: lr: 0.003987 
2024-01-10 07:21:09.374774: This epoch took 512.868016 s
 
2024-01-10 07:21:09.374850: 
epoch:  32 
2024-01-10 07:29:07.891228: train loss : -0.8708 
2024-01-10 07:29:41.451358: validation loss: -0.8447 
2024-01-10 07:29:41.451881: Average global foreground Dice: [0.9036, 0.947, 0.9338] 
2024-01-10 07:29:41.451994: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:29:42.069704: lr: 0.003787 
2024-01-10 07:29:42.069884: This epoch took 512.694963 s
 
2024-01-10 07:29:42.069966: 
epoch:  33 
2024-01-10 07:37:39.941523: train loss : -0.8712 
2024-01-10 07:38:13.713071: validation loss: -0.8445 
2024-01-10 07:38:13.713620: Average global foreground Dice: [0.9078, 0.9468, 0.9331] 
2024-01-10 07:38:13.713763: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:38:14.446642: lr: 0.003586 
2024-01-10 07:38:14.484430: saving checkpoint... 
2024-01-10 07:38:15.321537: done, saving took 0.87 seconds 
2024-01-10 07:38:15.322343: This epoch took 513.252267 s
 
2024-01-10 07:38:15.322415: 
epoch:  34 
2024-01-10 07:46:13.210998: train loss : -0.8728 
2024-01-10 07:46:46.949013: validation loss: -0.8473 
2024-01-10 07:46:46.949866: Average global foreground Dice: [0.9075, 0.947, 0.9338] 
2024-01-10 07:46:46.950020: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:46:47.716351: lr: 0.003384 
2024-01-10 07:46:47.757226: saving checkpoint... 
2024-01-10 07:46:48.357588: done, saving took 0.64 seconds 
2024-01-10 07:46:48.358375: This epoch took 513.035822 s
 
2024-01-10 07:46:48.358448: 
epoch:  35 
2024-01-10 07:54:47.119746: train loss : -0.8721 
2024-01-10 07:55:20.823525: validation loss: -0.8449 
2024-01-10 07:55:20.824166: Average global foreground Dice: [0.9063, 0.9464, 0.933] 
2024-01-10 07:55:20.824312: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 07:55:21.553751: lr: 0.00318 
2024-01-10 07:55:21.553990: This epoch took 513.195433 s
 
2024-01-10 07:55:21.554144: 
epoch:  36 
2024-01-10 08:03:19.908138: train loss : -0.8737 
2024-01-10 08:03:53.589290: validation loss: -0.8465 
2024-01-10 08:03:53.589816: Average global foreground Dice: [0.9079, 0.9467, 0.9349] 
2024-01-10 08:03:53.589928: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:03:54.481026: lr: 0.002975 
2024-01-10 08:03:54.496534: saving checkpoint... 
2024-01-10 08:03:55.070806: done, saving took 0.59 seconds 
2024-01-10 08:03:55.072116: This epoch took 513.517841 s
 
2024-01-10 08:03:55.072177: 
epoch:  37 
2024-01-10 08:11:52.867068: train loss : -0.8739 
2024-01-10 08:12:26.638483: validation loss: -0.8457 
2024-01-10 08:12:26.639001: Average global foreground Dice: [0.9062, 0.9475, 0.9334] 
2024-01-10 08:12:26.639108: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:12:27.294134: lr: 0.002768 
2024-01-10 08:12:27.310854: saving checkpoint... 
2024-01-10 08:12:27.877167: done, saving took 0.58 seconds 
2024-01-10 08:12:27.877755: This epoch took 512.805497 s
 
2024-01-10 08:12:27.877808: 
epoch:  38 
2024-01-10 08:20:26.858947: train loss : -0.8747 
2024-01-10 08:21:00.523733: validation loss: -0.8469 
2024-01-10 08:21:00.524308: Average global foreground Dice: [0.9063, 0.9481, 0.9344] 
2024-01-10 08:21:00.524421: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:21:01.246140: lr: 0.00256 
2024-01-10 08:21:01.260203: saving checkpoint... 
2024-01-10 08:21:01.850747: done, saving took 0.60 seconds 
2024-01-10 08:21:01.851521: This epoch took 513.973645 s
 
2024-01-10 08:21:01.851576: 
epoch:  39 
2024-01-10 08:29:00.590233: train loss : -0.8747 
2024-01-10 08:29:34.314614: validation loss: -0.8475 
2024-01-10 08:29:34.315113: Average global foreground Dice: [0.9079, 0.9474, 0.9345] 
2024-01-10 08:29:34.315215: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:29:34.978400: lr: 0.002349 
2024-01-10 08:29:34.995109: saving checkpoint... 
2024-01-10 08:29:35.556240: done, saving took 0.58 seconds 
2024-01-10 08:29:35.557166: This epoch took 513.705541 s
 
2024-01-10 08:29:35.557261: 
epoch:  40 
2024-01-10 08:37:33.219622: train loss : -0.8757 
2024-01-10 08:38:06.799533: validation loss: -0.8469 
2024-01-10 08:38:06.799990: Average global foreground Dice: [0.908, 0.9477, 0.9338] 
2024-01-10 08:38:06.800087: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:38:07.449479: lr: 0.002137 
2024-01-10 08:38:07.464035: saving checkpoint... 
2024-01-10 08:38:08.042267: done, saving took 0.59 seconds 
2024-01-10 08:38:08.043019: This epoch took 512.485624 s
 
2024-01-10 08:38:08.043088: 
epoch:  41 
2024-01-10 08:46:08.869469: train loss : -0.8760 
2024-01-10 08:46:42.577075: validation loss: -0.8491 
2024-01-10 08:46:42.577594: Average global foreground Dice: [0.9101, 0.948, 0.9345] 
2024-01-10 08:46:42.577689: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:46:43.177280: lr: 0.001922 
2024-01-10 08:46:43.190304: saving checkpoint... 
2024-01-10 08:46:43.752643: done, saving took 0.58 seconds 
2024-01-10 08:46:43.753577: This epoch took 515.710393 s
 
2024-01-10 08:46:43.753642: 
epoch:  42 
2024-01-10 08:54:42.659217: train loss : -0.8764 
2024-01-10 08:55:16.254637: validation loss: -0.8478 
2024-01-10 08:55:16.255355: Average global foreground Dice: [0.9083, 0.948, 0.9341] 
2024-01-10 08:55:16.255486: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 08:55:16.992359: lr: 0.001704 
2024-01-10 08:55:17.008087: saving checkpoint... 
2024-01-10 08:55:19.246606: done, saving took 2.25 seconds 
2024-01-10 08:55:19.273802: This epoch took 515.520057 s
 
2024-01-10 08:55:19.274035: 
epoch:  43 
2024-01-10 09:03:17.682464: train loss : -0.8770 
2024-01-10 09:03:51.592280: validation loss: -0.8451 
2024-01-10 09:03:51.592785: Average global foreground Dice: [0.9055, 0.9472, 0.9338] 
2024-01-10 09:03:51.592879: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:03:52.165930: lr: 0.001483 
2024-01-10 09:03:52.166128: This epoch took 512.891963 s
 
2024-01-10 09:03:52.166223: 
epoch:  44 
2024-01-10 09:11:51.774945: train loss : -0.8777 
2024-01-10 09:12:25.563686: validation loss: -0.8466 
2024-01-10 09:12:25.564377: Average global foreground Dice: [0.9084, 0.9478, 0.9344] 
2024-01-10 09:12:25.564533: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:12:26.259880: lr: 0.001259 
2024-01-10 09:12:26.281239: saving checkpoint... 
2024-01-10 09:12:26.852928: done, saving took 0.59 seconds 
2024-01-10 09:12:26.853963: This epoch took 514.687594 s
 
2024-01-10 09:12:26.854086: 
epoch:  45 
2024-01-10 09:20:26.391482: train loss : -0.8772 
2024-01-10 09:21:00.169569: validation loss: -0.8482 
2024-01-10 09:21:00.170180: Average global foreground Dice: [0.9079, 0.9486, 0.9342] 
2024-01-10 09:21:00.170322: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:21:01.017807: lr: 0.00103 
2024-01-10 09:21:01.037640: saving checkpoint... 
2024-01-10 09:21:01.601694: done, saving took 0.58 seconds 
2024-01-10 09:21:01.602494: This epoch took 514.748324 s
 
2024-01-10 09:21:01.602570: 
epoch:  46 
2024-01-10 09:29:01.255939: train loss : -0.8782 
2024-01-10 09:29:34.996950: validation loss: -0.8462 
2024-01-10 09:29:34.997490: Average global foreground Dice: [0.9063, 0.947, 0.9337] 
2024-01-10 09:29:34.997596: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:29:35.628768: lr: 0.000795 
2024-01-10 09:29:35.628952: This epoch took 514.026285 s
 
2024-01-10 09:29:35.629036: 
epoch:  47 
2024-01-10 09:37:35.112351: train loss : -0.8782 
2024-01-10 09:38:08.840236: validation loss: -0.8501 
2024-01-10 09:38:08.840861: Average global foreground Dice: [0.9113, 0.9484, 0.9351] 
2024-01-10 09:38:08.840974: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:38:09.494240: lr: 0.000552 
2024-01-10 09:38:09.513392: saving checkpoint... 
2024-01-10 09:38:10.053203: done, saving took 0.56 seconds 
2024-01-10 09:38:10.053959: This epoch took 514.424841 s
 
2024-01-10 09:38:10.054064: 
epoch:  48 
2024-01-10 09:46:09.372287: train loss : -0.8780 
2024-01-10 09:46:43.173602: validation loss: -0.8460 
2024-01-10 09:46:43.174175: Average global foreground Dice: [0.9079, 0.9475, 0.9345] 
2024-01-10 09:46:43.174301: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:46:43.888373: lr: 0.000296 
2024-01-10 09:46:43.905392: saving checkpoint... 
2024-01-10 09:46:44.507540: done, saving took 0.62 seconds 
2024-01-10 09:46:44.508476: This epoch took 514.454281 s
 
2024-01-10 09:46:44.508551: 
epoch:  49 
2024-01-10 09:54:44.734136: train loss : -0.8789 
2024-01-10 09:55:18.692191: validation loss: -0.8473 
2024-01-10 09:55:18.692747: Average global foreground Dice: [0.9093, 0.9482, 0.9337] 
2024-01-10 09:55:18.692848: (interpret this as an estimate for the Dice of the different classes. This is not exact.) 
2024-01-10 09:55:19.282732: lr: 0.0 
2024-01-10 09:55:19.282891: saving scheduled checkpoint file... 
2024-01-10 09:55:19.298613: saving checkpoint... 
2024-01-10 09:55:19.499649: done, saving took 0.22 seconds 
2024-01-10 09:55:19.500446: done 
2024-01-10 09:55:19.520403: saving checkpoint... 
2024-01-10 09:55:20.046969: done, saving took 0.55 seconds 
2024-01-10 09:55:20.048473: This epoch took 515.539849 s
 
2024-01-10 09:55:20.067455: saving checkpoint... 
2024-01-10 09:55:20.585626: done, saving took 0.54 seconds 
2024-01-10 09:55:48.854787: finished prediction 
2024-01-10 09:55:48.855277: evaluation of raw predictions 
2024-01-10 09:55:50.072049: determining postprocessing 
